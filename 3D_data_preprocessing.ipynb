{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Data Preprocessing\n",
    "### Brendon & Alec Barrios     |     08/08/2020\n",
    "Using OSIC Pulmonary Fibrosis Progression dataset from Kaggle.com\n",
    "\n",
    "<https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/overview>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: <https://www.kaggle.com/sentdex/first-pass-through-data-w-3d-convnet>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"train/\"\n",
    "patients = os.listdir(DATA_DIR)\n",
    "train_image_files = sorted(glob.glob(os.path.join(DATA_DIR, '*','*.dcm')))\n",
    "\n",
    "labels_df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "# labels_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_SIZE = 80     # recommended minimum: 70\n",
    "NUM_SLICES = 30   # recommended minimum: 20\n",
    "SAVE_FILE = \"{}data-{}x{}x{}.npy\".format(\"masked-train\", IMG_SIZE, IMG_SIZE, NUM_SLICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "from skimage import measure, morphology, segmentation\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.segmentation import clear_border\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def make_lungmask(img, display=False):\n",
    "    row_size= img.shape[0]\n",
    "    col_size = img.shape[1]\n",
    "    \n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    img = img-mean\n",
    "    img = img/std\n",
    "    \n",
    "    # Find the average pixel value near the lungs\n",
    "    # to renormalize washed out images\n",
    "    middle = img[int(col_size/5):int(col_size/5*4),int(row_size/5):int(row_size/5*4)] \n",
    "    mean = np.mean(middle)  \n",
    "    max = np.max(img)\n",
    "    min = np.min(img)\n",
    "    \n",
    "    # To improve threshold finding, I'm moving the \n",
    "    # underflow and overflow on the pixel spectrum\n",
    "    img[img==max]=mean\n",
    "    img[img==min]=mean\n",
    "    \n",
    "    # Using Kmeans to separate foreground (soft tissue / bone) and background (lung/air)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=2).fit(np.reshape(middle,[np.prod(middle.shape),1]))\n",
    "    centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "    threshold = np.mean(centers)\n",
    "    thresh_img = np.where(img<threshold,1.0,0.0)  # threshold the image\n",
    "\n",
    "    # First erode away the finer elements, then dilate to include some of the pixels surrounding the lung.  \n",
    "    # We don't want to accidentally clip the lung.\n",
    "\n",
    "    eroded = morphology.erosion(thresh_img,np.ones([3,3]))\n",
    "    dilation = morphology.dilation(eroded,np.ones([8,8]))\n",
    "\n",
    "    labels = measure.label(dilation) # Different labels are displayed in different colors\n",
    "    label_vals = np.unique(labels)\n",
    "    regions = measure.regionprops(labels)\n",
    "    good_labels = []\n",
    "    for prop in regions:\n",
    "        B = prop.bbox\n",
    "        if B[2]-B[0]<row_size/10*9 and B[3]-B[1]<col_size/10*9 and B[0]>row_size/5 and B[2]<col_size/5*4:\n",
    "            good_labels.append(prop.label)\n",
    "    mask = np.ndarray([row_size,col_size],dtype=np.int8)\n",
    "    mask[:] = 0\n",
    "\n",
    "\n",
    "    #  After just the lungs are left, we do another large dilation\n",
    "    #  in order to fill in and out the lung mask \n",
    "    \n",
    "    for N in good_labels:\n",
    "        mask = mask + np.where(labels==N,1,0)\n",
    "    mask = morphology.dilation(mask,np.ones([10,10])) # one last dilation\n",
    "\n",
    "    if (display):\n",
    "        fig, ax = plt.subplots(3, 2, figsize=[12, 12])\n",
    "        ax[0, 0].set_title(\"Original\")\n",
    "        ax[0, 0].imshow(img, cmap='gray')\n",
    "        ax[0, 0].axis('off')\n",
    "        ax[0, 1].set_title(\"Threshold\")\n",
    "        ax[0, 1].imshow(thresh_img, cmap='gray')\n",
    "        ax[0, 1].axis('off')\n",
    "        ax[1, 0].set_title(\"After Erosion and Dilation\")\n",
    "        ax[1, 0].imshow(dilation, cmap='gray')\n",
    "        ax[1, 0].axis('off')\n",
    "        ax[1, 1].set_title(\"Color Labels\")\n",
    "        ax[1, 1].imshow(labels)\n",
    "        ax[1, 1].axis('off')\n",
    "        ax[2, 0].set_title(\"Final Mask\")\n",
    "        ax[2, 0].imshow(mask, cmap='gray')\n",
    "        ax[2, 0].axis('off')\n",
    "        ax[2, 1].set_title(\"Apply Mask on Original\")\n",
    "        ax[2, 1].imshow(mask*img, cmap='gray')\n",
    "        ax[2, 1].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "    return mask*img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "def normalize(a):\n",
    "    a[a < 0] = 0\n",
    "    norm = np.linalg.norm(a)\n",
    "    if norm == 0:\n",
    "        return a\n",
    "    return a / norm\n",
    "\n",
    "\n",
    "def process_data(patient, labels_df, img_px_size=70, num_slices=20, visualize=False):\n",
    "    \n",
    "    print(patient)\n",
    "    path = os.path.join(DATA_DIR, patient)\n",
    "    slices = [pydicom.read_file(os.path.join(path, file)) for file in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2])) # sorts images serially\n",
    "    \n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(make_lungmask(each_slice.pixel_array), (img_px_size, img_px_size)) for each_slice in slices]\n",
    "           \n",
    "    chunk_step = math.ceil(len(slices) / num_slices) # (num_slices^2 + 1)/num_slices\n",
    "    \n",
    "    # Chunks and averages images for patients with > NUM_SLICES images\n",
    "    for slice_chunk in chunks(slices, chunk_step):\n",
    "        avg_slice = np.array(list(map(mean, zip(*slice_chunk))))\n",
    "        new_slices.append(avg_slice)\n",
    "#         new_slices.append(make_lungmask(avg_slice))\n",
    "    \"\"\"\n",
    "    probably the best place to apply mask ^^^, but will need to rework which images are allowed in the chunks\n",
    "    \"\"\" \n",
    "    \n",
    "    print(\"Chunked Slices:\", len(new_slices))    \n",
    "    \n",
    "    # Handle data with less than num_slices images\n",
    "    diff = num_slices - len(new_slices)\n",
    "    if diff:\n",
    "        for n in range(diff):\n",
    "            mid = int(len(new_slices) / 2)\n",
    "            # possibly mirror the duplicate images(?)\n",
    "            new_slices.append(new_slices[mid])\n",
    "        \n",
    "    # Handle data with more than num_slices images\n",
    "    while len(new_slices) > num_slices:\n",
    "        new_img = list(map(mean, zip(*[new_slices[-1], new_slices[-2]])))\n",
    "        del new_slices[num_slices]\n",
    "        new_slices[num_slices - 1] = new_img\n",
    "        \n",
    "    print(\"  Final Slices:\", len(new_slices))\n",
    "    \n",
    "    if visualize:\n",
    "        cols = int(num_slices / 5)\n",
    "        fig = plt.figure(figsize=(16,12)) # double the default figsize\n",
    "        for num, each_slice in enumerate(new_slices):\n",
    "            y = fig.add_subplot(5, cols, num+1)\n",
    "            y.imshow(each_slice, cmap='gray')\n",
    "        plt.show\n",
    "        \n",
    "    '''\n",
    "    Obtain slope of FVC Score as label\n",
    "    Possibly check R^2 values as a counterpart to confidence score\n",
    "    Check if non-linear functions describe FVC trends better than linear functions\n",
    "    If so, brainstorm a numeric readout (to replace slope) from the non-linear f(x) that works best\n",
    "    '''\n",
    "    FVC = np.array(labels_df.at[patient, \"FVC\"])\n",
    "    Weeks = np.array(labels_df.at[patient, \"Weeks\"])\n",
    "    m, b = np.polyfit(Weeks, FVC, 1) # y = m*x + b; where x = Weeks, and y = FVC\n",
    "        \n",
    "    return np.array(new_slices), -m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID00007637202177411956430\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00009637202177434476278\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00010637202177584971671\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00011637202177653955184\n",
      "GDCM required!\n",
      "ID00012637202177665765362\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00014637202177757139317\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00015637202177877247924\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00019637202178323708467\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00020637202178344345685\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00023637202179104603099\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00025637202179541264076\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00026637202179561894768\n",
      "Missing ImagePositionPatient!\n",
      "ID00027637202179689871102\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00030637202181211009029\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00032637202181710233084\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00035637202182204917484\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00038637202182690843176\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00042637202184406822975\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00047637202184938901501\n",
      "Chunked Slices: 26\n",
      "  Final Slices: 30\n",
      "ID00048637202185016727717\n",
      "Chunked Slices: 26\n",
      "  Final Slices: 30\n",
      "ID00051637202185848464638\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00052637202186188008618\n",
      "GDCM required!\n",
      "ID00060637202187965290703\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00061637202188184085559\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00062637202188654068490\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00067637202189903532242\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00068637202190879923934\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00072637202198161894406\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00073637202198167792918\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00075637202198610425520\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00076637202199015035026\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00077637202199102000916\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00078637202199415319443\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00082637202201836229724\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00086637202203494931510\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00089637202204675567570\n",
      "Chunked Slices: 18\n",
      "  Final Slices: 30\n",
      "ID00090637202204766623410\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00093637202205278167493\n",
      "Chunked Slices: 19\n",
      "  Final Slices: 30\n",
      "ID00094637202205333947361\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00099637202206203080121\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00102637202206574119190\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00104637202208063407045\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00105637202208831864134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8d6c6b933357>:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  img = img/std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided by NaN/âˆž!\n",
      "ID00108637202209619669361\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00109637202210454292264\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00110637202210673668310\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00111637202210956877205\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00115637202211874187958\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00117637202212360228007\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00119637202215426335765\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00122637202216437668965\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00123637202217151272140\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00124637202217596410344\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00125637202218590429387\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00126637202218610655908\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "ID00127637202219096738943\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00128637202219474716089\n",
      "Missing ImagePositionPatient!\n",
      "ID00129637202219868188000\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00130637202220059448013\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00131637202220424084844\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00132637202222178761324\n",
      "Missing ImagePositionPatient!\n",
      "ID00133637202223847701934\n",
      "Chunked Slices: 26\n",
      "  Final Slices: 30\n",
      "ID00134637202223873059688\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00135637202224630271439\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00136637202224951350618\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00138637202231603868088\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00139637202231703564336\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00140637202231728595149\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00149637202232704462834\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00161637202235731948764\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00165637202237320314458\n",
      "Chunked Slices: 12\n",
      "  Final Slices: 30\n",
      "ID00167637202237397919352\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00168637202237852027833\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00169637202238024117706\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00170637202238079193844\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00172637202238316925179\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00173637202238329754031\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00180637202240177410333\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00183637202241995351650\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00184637202242062969203\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00186637202242472088675\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00190637202244450116191\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00192637202245493238298\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00196637202246668775836\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00197637202246865691526\n",
      "Chunked Slices: 26\n",
      "  Final Slices: 30\n",
      "ID00199637202248141386743\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00202637202249376026949\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00207637202252526380974\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00210637202257228694086\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00213637202257692916109\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00214637202257820847190\n",
      "Chunked Slices: 18\n",
      "  Final Slices: 30\n",
      "ID00216637202257988213445\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00218637202258156844710\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00219637202258203123958\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00221637202258717315571\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00222637202259066229764\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00224637202259281193413\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00225637202259339837603\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00228637202259965313869\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00229637202260254240583\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "100\n",
      "ID00232637202260377586117\n",
      "Chunked Slices: 23\n",
      "  Final Slices: 30\n",
      "ID00233637202260580149633\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00234637202261078001846\n",
      "Chunked Slices: 19\n",
      "  Final Slices: 30\n",
      "ID00235637202261451839085\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00240637202264138860065\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "ID00241637202264294508775\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00242637202264759739921\n",
      "Chunked Slices: 18\n",
      "  Final Slices: 30\n",
      "ID00248637202266698862378\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00249637202266730854017\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00251637202267455595113\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00255637202267923028520\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00264637202270643353440\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00267637202270790561585\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00273637202271319294586\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00275637202271440119890\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00276637202271694539978\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00279637202272164826258\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00283637202278714365037\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00285637202278913507108\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00288637202279148973731\n",
      "Chunked Slices: 26\n",
      "  Final Slices: 30\n",
      "ID00290637202279304677843\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00291637202279398396106\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00294637202279614924243\n",
      "Chunked Slices: 18\n",
      "  Final Slices: 30\n",
      "ID00296637202279895784347\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00298637202280361773446\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "ID00299637202280383305867\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00305637202281772703145\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00307637202282126172865\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00309637202282195513787\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00312637202282607344793\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00317637202283194142136\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00319637202283897208687\n",
      "Chunked Slices: 19\n",
      "  Final Slices: 30\n",
      "ID00322637202284842245491\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00323637202285211956970\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00329637202285906759848\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00331637202286306023714\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00335637202286784464927\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00336637202286801879145\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "ID00337637202286839091062\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00339637202287377736231\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00340637202287399835821\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00341637202287410878488\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00342637202287526592911\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00343637202287577133798\n",
      "Chunked Slices: 20\n",
      "  Final Slices: 30\n",
      "ID00344637202287684217717\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00351637202289476567312\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00355637202295106567614\n",
      "Chunked Slices: 19\n",
      "  Final Slices: 30\n",
      "ID00358637202295388077032\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00360637202295712204040\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00364637202296074419422\n",
      "Chunked Slices: 17\n",
      "  Final Slices: 30\n",
      "ID00365637202296085035729\n",
      "Chunked Slices: 22\n",
      "  Final Slices: 30\n",
      "ID00367637202296290303449\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00368637202296470751086\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00370637202296737666151\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00371637202296828615743\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00376637202297677828573\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00378637202298597306391\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00381637202299644114027\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00383637202300493233675\n",
      "Chunked Slices: 16\n",
      "  Final Slices: 30\n",
      "ID00388637202301028491611\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00392637202302319160044\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00393637202302431697467\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00398637202303897337979\n",
      "Chunked Slices: 25\n",
      "  Final Slices: 30\n",
      "ID00400637202305055099402\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00401637202305320178010\n",
      "Chunked Slices: 24\n",
      "  Final Slices: 30\n",
      "ID00405637202308359492977\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00407637202308788732304\n",
      "Chunked Slices: 27\n",
      "  Final Slices: 30\n",
      "ID00408637202308839708961\n",
      "Chunked Slices: 18\n",
      "  Final Slices: 30\n",
      "ID00411637202309374271828\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00414637202310318891556\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00417637202310901214011\n",
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00419637202311204720264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Slices: 28\n",
      "  Final Slices: 30\n",
      "ID00421637202311550012437\n",
      "Chunked Slices: 21\n",
      "  Final Slices: 30\n",
      "ID00422637202311677017371\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "ID00423637202312137826377\n",
      "Chunked Slices: 29\n",
      "  Final Slices: 30\n",
      "ID00426637202313170790466\n",
      "Chunked Slices: 30\n",
      "  Final Slices: 30\n",
      "Save successful!\n",
      "Runtime Errors: 2\n",
      "Attribute Errors: 3\n",
      "Value Errors: 1\n"
     ]
    }
   ],
   "source": [
    "much_data = []\n",
    "num_key_errors = 0\n",
    "num_run_errors = 0\n",
    "num_val_errors = 0\n",
    "num_attribute_errors = 0\n",
    "\n",
    "for num, patient in enumerate(patients):\n",
    "    if num%100 == 0 and num != 0:\n",
    "        print(num)\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a work-around for files that require GDCM to read. Try implementing get_pixeldata() method.\n",
    "    \"\"\"      \n",
    "    try:\n",
    "        img_data, label, intercept = process_data(patient, \n",
    "                                                  labels_df, \n",
    "                                                  img_px_size=IMG_SIZE, \n",
    "                                                  num_slices=NUM_SLICES, \n",
    "                                                  visualize=False)\n",
    "        \n",
    "        much_data.append([img_data, label, intercept])\n",
    "        \n",
    "    except KeyError:\n",
    "        print(\"Unlabeled data!\") # error that Sentdex handled\n",
    "        num_key_errors += 1\n",
    "        \n",
    "    except RuntimeError:\n",
    "        print(\"GDCM required!\") # error that requires GDCM dependency\n",
    "        num_run_errors += 1\n",
    "        \n",
    "    except AttributeError:\n",
    "        print(\"Missing ImagePositionPatient!\") \n",
    "        num_attribute_errors += 1\n",
    "        \n",
    "    except ValueError:\n",
    "        print(u\"Divided by NaN/\\N{Infinity}!\")\n",
    "        num_val_errors += 1\n",
    "        \n",
    "    except Error as e:\n",
    "        print(\"Other error:\", e)\n",
    "        \n",
    "np.save(SAVE_FILE, much_data, allow_pickle=True)\n",
    "print(\"Save successful!\")\n",
    "\n",
    "if num_key_errors:\n",
    "    print(\"KeyErrors: {}\".format(num_key_errors))\n",
    "if num_run_errors:\n",
    "    print(\"Runtime Errors: {}\".format(num_run_errors))\n",
    "if num_attribute_errors:\n",
    "    print(\"Attribute Errors: {}\".format(num_attribute_errors))\n",
    "if num_val_errors:\n",
    "    print(\"Value Errors: {}\".format(num_val_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC = np.array(labels_df.at[patient, \"FVC\"])\n",
    "Weeks = np.array(labels_df.at[patient, \"Weeks\"])\n",
    "m, b = np.polyfit(Weeks, FVC, 1)\n",
    "plt.plot(Weeks, FVC, 'o')\n",
    "plt.plot(Weeks, m*Weeks + b)\n",
    "print(m)\n",
    "print(-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof of Save/Load states:\n",
    "\"\"\"\n",
    "images = np.load(SAVE_FILE, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Raw Data: \", images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Normalized Image Data:\\n\", images[-1,0][1])\n",
    "plt.figure()\n",
    "plt.imshow(images[-1,0][10], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of concept: Loading in training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "b = []\n",
    "for n in range(len(images)):\n",
    "    x_train.append(images[n, 0])\n",
    "    y_train.append(images[n, 1])\n",
    "    b.append(images[n, 2])\n",
    "X = np.array(x_train)\n",
    "Y = np.array(y_train)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "NUM_SLICES*IMG_SIZE**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders And Lung Fibrosis Code\n",
    "<https://www.kaggle.com/digvijayyadav/autoencoders-and-lung-fibrosis/notebook>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_scan(path):\n",
    "    \"\"\"\n",
    "    Loads scans from a folder and into a list.\n",
    "    \n",
    "    Parameters: path (Folder path)\n",
    "    \n",
    "    Returns: slices (List of slices)\n",
    "    \"\"\"\n",
    "    \n",
    "    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.InstanceNumber))\n",
    "    \n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "        \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "        \n",
    "    return slices\n",
    "\n",
    "\n",
    "def get_pixels_hu(scans):\n",
    "    \"\"\"\n",
    "    Converts raw images to Hounsfield Units (HU).\n",
    "    \n",
    "    Parameters: scans (Raw images)\n",
    "    \n",
    "    Returns: image (NumPy array)\n",
    "    \"\"\"\n",
    "    \n",
    "    image = np.stack([s.pixel_array for s in scans])\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Since the scanning equipment is cylindrical in nature and image output is square,\n",
    "    # we set the out-of-scan pixels to 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    \n",
    "    # HU = m*P + b\n",
    "    intercept = scans[0].RescaleIntercept\n",
    "    slope = scans[0].RescaleSlope\n",
    "    \n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "        \n",
    "    image += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "\n",
    "test_patient_scans = load_scan(DATA_DIR + patients[2])\n",
    "test_patient_images = get_pixels_hu(test_patient_scans)\n",
    "\n",
    "#We'll be taking a random slice to perform segmentation:\n",
    "\n",
    "for imgs in range(len(test_patient_images[0:5])):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,15))\n",
    "    ax1.imshow(test_patient_images[imgs], cmap='gray')\n",
    "    ax1.set_title(\"Original Slice\")\n",
    "    \n",
    "    ax2.imshow(test_patient_images[imgs], cmap='gray')\n",
    "    ax2.set_title(\"Original Slice\")\n",
    "    \n",
    "    ax3.imshow(test_patient_images[imgs], cmap='gray')\n",
    "    ax3.set_title(\"Original Slice\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markers(image):\n",
    "    \"\"\"\n",
    "    Generates markers for a given image.\n",
    "    \n",
    "    Parameters: image\n",
    "    \n",
    "    Returns: Internal Marker, External Marker, Watershed Marker\n",
    "    \"\"\"\n",
    "    \n",
    "    #Creation of the internal Marker\n",
    "    marker_internal = image < -400\n",
    "    marker_internal = segmentation.clear_border(marker_internal)\n",
    "    marker_internal_labels = measure.label(marker_internal)\n",
    "    \n",
    "    areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n",
    "    areas.sort()\n",
    "    \n",
    "    if len(areas) > 2:\n",
    "        for region in measure.regionprops(marker_internal_labels):\n",
    "            if region.area < areas[-2]:\n",
    "                for coordinates in region.coords:                \n",
    "                       marker_internal_labels[coordinates[0], coordinates[1]] = 0\n",
    "    \n",
    "    marker_internal = marker_internal_labels > 0\n",
    "    \n",
    "    # Creation of the External Marker\n",
    "    external_a = ndimage.binary_dilation(marker_internal, iterations=10)\n",
    "    external_b = ndimage.binary_dilation(marker_internal, iterations=55)\n",
    "    marker_external = external_b ^ external_a\n",
    "    \n",
    "    # Creation of the Watershed Marker\n",
    "    marker_watershed = np.zeros((512, 512), dtype=np.int)\n",
    "    marker_watershed += marker_internal * 255\n",
    "    marker_watershed += marker_external * 128\n",
    "    \n",
    "    return marker_internal, marker_external, marker_watershed\n",
    "\n",
    "test_patient_internal, test_patient_external, test_patient_watershed = generate_markers(test_patient_images[15])\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,15))\n",
    "\n",
    "ax1.imshow(test_patient_internal, cmap='gray')\n",
    "ax1.set_title(\"Internal Marker\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(test_patient_external, cmap='gray')\n",
    "ax2.set_title(\"External Marker\")\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3.imshow(test_patient_watershed, cmap='gray')\n",
    "ax3.set_title(\"Watershed Marker\")\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = pydicom.dcmread(train_image_files[7])\n",
    "img = sample_image.pixel_array\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (img + sample_image.RescaleIntercept) / sample_image.RescaleSlope\n",
    "img = img < -400 #HU unit range for lungs CT SCAN\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Binary Mask Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = clear_border(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Cleaned Border Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = pydicom.dcmread(train_image_files[5])\n",
    "img = sample_image.pixel_array\n",
    "img = label(img)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = [r.area for r in regionprops(img)]\n",
    "areas.sort()\n",
    "if len(areas) > 2:\n",
    "    for region in regionprops(img):\n",
    "        if region.area < areas[-2]:\n",
    "            for coordinates in region.coords:                \n",
    "                img[coordinates[0], coordinates[1]] = 0\n",
    "img = img > 0\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_image = pydicom.dcmread(train_image_files[5])\n",
    "img = sample_image.pixel_array\n",
    "\n",
    "mask_img = make_lungmask(img, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
