{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "### Brendon & Alec Barrios     |     08/08/2020\n",
    "Using OSIC Pulmonary Fibrosis Progression dataset from Kaggle.com\n",
    "\n",
    "<https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/overview>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: <https://www.kaggle.com/sentdex/first-pass-through-data-w-3d-convnet>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID00419637202311204720264</th>\n",
       "      <td>6</td>\n",
       "      <td>3020</td>\n",
       "      <td>70.186855</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID00421637202311550012437</th>\n",
       "      <td>15</td>\n",
       "      <td>2739</td>\n",
       "      <td>82.045291</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID00422637202311677017371</th>\n",
       "      <td>6</td>\n",
       "      <td>1930</td>\n",
       "      <td>76.672493</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID00423637202312137826377</th>\n",
       "      <td>17</td>\n",
       "      <td>3294</td>\n",
       "      <td>79.258903</td>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID00426637202313170790466</th>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Weeks   FVC    Percent  Age   Sex SmokingStatus\n",
       "Patient                                                                   \n",
       "ID00419637202311204720264      6  3020  70.186855   73  Male     Ex-smoker\n",
       "ID00421637202311550012437     15  2739  82.045291   68  Male     Ex-smoker\n",
       "ID00422637202311677017371      6  1930  76.672493   73  Male     Ex-smoker\n",
       "ID00423637202312137826377     17  3294  79.258903   72  Male     Ex-smoker\n",
       "ID00426637202313170790466      0  2925  71.824968   73  Male  Never smoked"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"train_dl:\"\n",
    "patients = os.listdir(DATA_DIR)\n",
    "\n",
    "labels_df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "\n",
    "labels_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "IMAGE_PX_SIZE = 80  # recommended minimum: 70\n",
    "NUM_SLICES = 30     # recommended minimum: 20\n",
    "SAVE_FILE = \"traindata-{}x{}x{}.npy\".format(IMAGE_PX_SIZE, IMAGE_PX_SIZE, NUM_SLICES)\n",
    "\n",
    "def chunks(l, n):\n",
    "    # Credit: Ned Batchelder\n",
    "    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "def normalize(a):\n",
    "    norm = np.linalg.norm(a)\n",
    "    if norm == 0:\n",
    "        return a\n",
    "    return a / norm\n",
    "\n",
    "def process_data(patient, labels_df, img_px_size=70, num_slices=20, visualize=False):\n",
    "    \n",
    "    path = os.path.join(DATA_DIR, patient)\n",
    "    slices = [pydicom.read_file(os.path.join(path, file)) for file in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: int(x.ImagePositionPatient[2])) # sorts dicom files by Image Position\n",
    "#     print(slices[0])\n",
    "    \n",
    "    new_slices = []\n",
    "    slices = [cv2.resize(normalize(each_slice.pixel_array), (img_px_size, img_px_size)) for each_slice in slices]\n",
    "           \n",
    "    chunk_step = math.ceil(len(slices) / num_slices) # (num_slices^2 + 1)/num_slices\n",
    "    \n",
    "    # Chunks and averages images for patients with >40 images\n",
    "    for slice_chunk in chunks(slices, chunk_step):\n",
    "        avg_slice = list(map(mean, zip(*slice_chunk)))\n",
    "        new_slices.append(avg_slice)\n",
    "    \n",
    "    print(len(new_slices))    \n",
    "    \n",
    "    # Handle data with less than num_slices images\n",
    "    diff = num_slices - len(new_slices)\n",
    "    if diff:\n",
    "        for n in range(diff):\n",
    "            mid = int(len(new_slices) / 2)\n",
    "            # possibly mirror the duplicate images(?)\n",
    "            new_slices.append(new_slices[mid])\n",
    "        \n",
    "    # Handle data with more than num_slices images\n",
    "    while len(new_slices) > num_slices:\n",
    "        new_img = list(map(mean, zip(*[new_slices[-1], new_slices[-2]])))\n",
    "        del new_slices[num_slices]\n",
    "        new_slices[num_slices - 1] = new_img\n",
    "        \n",
    "    print(len(new_slices))\n",
    "    \n",
    "    if visualize:\n",
    "        cols = int(num_slices / 5)\n",
    "        fig = plt.figure(figsize=(16,12)) # double the default figsize\n",
    "        for num, each_slice in enumerate(new_slices):\n",
    "            y = fig.add_subplot(5, cols, num+1)\n",
    "            y.imshow(each_slice, cmap = \"gray\")\n",
    "        plt.show\n",
    "        \n",
    "    '''\n",
    "    Obtain slope of FVC Score as label\n",
    "    Possibly check R^2 values as a counterpart to confidence score\n",
    "    Check if non-linear functions describe FVC trends better than linear functions\n",
    "    If so, brainstorm a numeric readout (to replace slope) from the non-linear f(x) that works best\n",
    "    '''\n",
    "    FVC = np.array(labels_df.at[patient, \"FVC\"])\n",
    "    wk = np.array(labels_df.at[patient, \"Weeks\"])\n",
    "    m, b = np.polyfit(wk, FVC, 1)\n",
    "        \n",
    "    return np.array(new_slices), -m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected 1D vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-90b42b0ad32e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                   \u001b[0mimg_px_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_PX_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                   \u001b[0mnum_slices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_SLICES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                                   visualize=False)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmuch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-11b09af3ea17>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(patient, labels_df, img_px_size, num_slices, visualize)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mFVC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FVC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mwk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Weeks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected deg >= 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 1D vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected non-empty vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected 1D vector for x"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try Preprocessing Data \n",
    "If memory fails: try to process \"online\", meaning call process_data() while network is training to feed data into network\n",
    "\"\"\"\n",
    "much_data = []\n",
    "\n",
    "for num, patient in enumerate(patients[:5]):\n",
    "    if num%100 == 0 and num != 0:\n",
    "        print(num)\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a work-around for files that require GDCM to read. Try using get_pixeldata() method to work.\n",
    "    \"\"\"      \n",
    "    try:\n",
    "        img_data, label, intercept = process_data(patient, \n",
    "                                                  labels_df, \n",
    "                                                  img_px_size=IMAGE_PX_SIZE, \n",
    "                                                  num_slices=NUM_SLICES, \n",
    "                                                  visualize=False)\n",
    "        \n",
    "        much_data.append([img_data, label, intercept])\n",
    "        \n",
    "    except KeyError:\n",
    "        print(\"Unlabeled data!\") # error that Sentdex handled\n",
    "        \n",
    "    except RuntimeError:\n",
    "        print(\"GDCM required!\") # error that we are encountering\n",
    "        \n",
    "np.save(SAVE_FILE, much_data, allow_pickle=True)\n",
    "print(\"Save successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC = np.array(labels_df.at[patient, \"FVC\"])\n",
    "wk = np.array(labels_df.at[patient, \"Weeks\"])\n",
    "m, b = np.polyfit(wk, FVC, 1)\n",
    "plt.plot(wk, FVC, 'o')\n",
    "plt.plot(wk, m*wk + b)\n",
    "print(m)\n",
    "print(-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof of Save/Load states:\n",
    "\"\"\"\n",
    "images = np.load(SAVE_FILE, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Raw Data: \", images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Normalized Image Data:\\n\", images[3,0][1])\n",
    "plt.figure()\n",
    "plt.imshow(images[3,0][25], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of concept: Loading in training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "b = []\n",
    "for n in range(len(images)):\n",
    "    x_train.append(images[n, 0])\n",
    "    y_train.append(images[n, 1])\n",
    "    b.append(images[n, 2])\n",
    "X = np.array(x_train)\n",
    "Y = np.array(y_train)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "50**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D_ConvNet Model: based on sentdex modified by Alec Barrios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonprogramming.net/cnn-tensorflow-convolutional-nerual-network-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-6bfb3f7fd3d8>, line 80)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-6bfb3f7fd3d8>\"\u001b[0;36m, line \u001b[0;32m80\u001b[0m\n\u001b[0;31m    train_neural_network(x)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nunmpy as np\n",
    "\n",
    "#IMAGE_PX_SIZE = 80  # created in earlier cell\n",
    "#NUM_SLICES = 30   \n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "keep_rate = 0.8\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])), #first argument is wrong\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, IMAGE_PX_SIZE, IMAGE_PX_SIZE, NUM_SLICES, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2,[-1, 7*7*64]) #second argument is wrong\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    \n",
    "    much_data = np.load(\"testdata-80x80x30.npy\")\n",
    "    train_data = much_data[:-100]\n",
    "    validation_data = much_data[-100:]\n",
    "    \n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 3\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for data in train_data:\n",
    "                X = data[0]\n",
    "                Y = data[1]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                epoch_loss +=c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
